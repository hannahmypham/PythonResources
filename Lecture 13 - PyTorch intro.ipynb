{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "077289d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 13: Introduction to Machine Learning with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa255b9a8d6821bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Goals\n",
    "* Understand the basic concepts of machine learning.\n",
    "* Know core PyTorch concepts: tensors, datasets, modules, autograd, optimisers.\n",
    "* Apply PyTorch to build and train a simple machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3867bb20",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Basic Concepts of Machine Learning\n",
    "* Machine learning involves training models to make predictions or decisions based on data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d10f0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/ML_overview.png\" alt=\"ML Overview schematic\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7f59e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* In this introduction, we will focus on **supervised learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4734369",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Background\n",
    "\n",
    "### What is Machine Learning?\n",
    "* Machine learning is a subset of artificial intelligence that focuses on building systems that can learn from and make decisions based on data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3a107c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Unlike traditional programming, where explicit instructions are provided, machine learning algorithms identify patterns in data and make predictions or decisions without being explicitly programmed for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d209cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Common applications of machine learning include:\n",
    "  - Image and speech recognition\n",
    "  - Natural language processing\n",
    "  - Recommender systems\n",
    "  - Fraud detection\n",
    "  - Autonomous vehicles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de17684",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regard Machine Learning as Function Approximation\n",
    "* At its core, machine learning can be viewed as a function approximation problem. (Here we take supervised learning as an example.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8201a41",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Given a dataset $D$ consisting of input-output pairs $(x_i, y_i)$, the goal is to learn a function $f(x; \\theta)$ parameterised by $\\theta$ that maps inputs $x$ to outputs $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9ba323",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The learning process involves finding the optimal parameters $\\theta^*$ that minimise a loss function $L(y, f(x; \\theta))$, which quantifies the difference between the predicted outputs and the true outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab39560d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The optimisation problem can be formulated as:\n",
    "$$\\theta^* = \\arg\\min_{\\theta} \\sum_{i=1}^{N} L(y_i, f(x_i; \\theta))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e29b40",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5535c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The optimisation is typically performed using gradient-based methods, such as Stochastic Gradient Descent (SGD) or Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b60e5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gradient Descent\n",
    "* Gradient descent is an optimisation algorithm used to minimise the loss function by iteratively updating the model parameters in the direction of the negative gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563a3b61",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The update rule for gradient descent is given by:\n",
    "$$\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_{\\theta} L(y, f(x; \\theta^{(t)}))$$\n",
    "where $\\eta$ is the learning rate, and $\\nabla_{\\theta} L(y, f(x; \\theta^{(t)}))$ is the gradient of the loss function with respect to the parameters $\\theta$ at iteration $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c4409",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Variants of gradient descent include:\n",
    "  - Batch Gradient Descent: Uses the entire dataset to compute the gradient.\n",
    "  - Stochastic Gradient Descent (SGD): Uses a single data point to compute the gradient.\n",
    "  - Mini-batch Gradient Descent: Uses a small batch of data points to compute the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4286cdf0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In practice, mini-batch gradient descent is commonly used as it balances the efficiency of batch gradient descent and the noise reduction of SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6dadf9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Machine Learning Framework\n",
    "\n",
    "* Key components of a machine learning workflow:\n",
    "  - Data Collection: Gathering relevant data for training.\n",
    "  - Data Preprocessing: Cleaning and transforming data into a suitable format.\n",
    "  - Model Selection: Choosing an appropriate algorithm or architecture.\n",
    "  - Training: Optimising the model parameters using training data.\n",
    "  - Validation: Assessing model performance on unseen data.\n",
    "  - Deployment: Integrating the model into a production environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ebcd3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Machine Learning Framework\n",
    "* A machine learning framework provides tools and libraries to facilitate the development, training, and deployment of machine learning models.\n",
    "* Popular machine learning frameworks include:\n",
    "  - PyTorch\n",
    "  - TensorFlow\n",
    "  - Scikit-learn\n",
    "  - Keras\n",
    "* In this introduction, we will focus on **PyTorch**, which is nowadays widely used for deep learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dec0a6c2b9369f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In this introduction, we will focus on the **training** and **validation** steps using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59547078a438d2c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "* We need to first set up a Python environment with the necessary libraries.\n",
    "* It is good practice to use virtual environments to manage dependencies. We will use `venv` to create a virtual environment and install PyTorch.\n",
    "* Note: the exclamation mark `!` below is used to run shell commands in Jupyter notebooks. When running in a terminal, you must **NOT** include it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf90e34169517d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m venv ml_env\n",
    "!source ml_env/bin/activate\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b34b8f49d41272",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* You can run PyTorch on both CPU and GPU. If you have a compatible NVIDIA GPU, you can install the CUDA version for better performance.\n",
    "* The following code block checks if PyTorch is installed correctly and whether a GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3a91d98023194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1467ff3ecd1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30604f9889626c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f31c6ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PyTorch Basics\n",
    "* PyTorch is a popular deep learning library that provides tools for building and training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbb48a13b21b4d9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Key concepts in PyTorch:\n",
    "  - Tensors: Multi-dimensional arrays that are the basic building blocks of PyTorch.\n",
    "  - Datasets and DataLoaders: Utilities for handling and loading data.\n",
    "  - Modules: Building blocks for neural networks (e.g., layers).\n",
    "  - Autograd: Automatic differentiation for computing gradients.\n",
    "  - Optimisers: Algorithms for updating model parameters during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55833386",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tensors\n",
    "* Tensors are similar to NumPy arrays but can also be used on GPUs for acceleration.\n",
    "* You can create tensors in various ways, such as from lists or using random values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1982234c59c273eb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Here is an example of creating a random tensor and checking its properties.\n",
    "* You can specify the device (CPU or GPU) and data type (e.g., float32, int64) when creating tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b19fab7dafa382",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32, 3, 244, 244, device=\"cpu\", dtype=torch.float32)\n",
    "print(\"Tensor:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a2c5155dcdab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tensor shape:\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541ea9dbb60546e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Tensor device:\", x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc337646b2323602",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tensor dtype:\", x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e3a8b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Datasets and DataLoaders\n",
    "* Datasets are used to represent a collection of data samples.\n",
    "* DataLoaders provide an efficient way to iterate over datasets in batches, with options for shuffling and parallel loading.\n",
    "* You can create custom datasets by subclassing `torch.utils.data.Dataset` with `__len__` and `__getitem__` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa190a948f8b2da8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Here is an example of a simple custom dataset and using a DataLoader to iterate over it in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9287c97c0ead3afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b62e066f2e01b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn(100, 10)\n",
    "print(\"Data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89bff4da8c59b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.randint(0, 2, (100,))\n",
    "print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52b2b4b1b3cfc4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dataset = SimpleDataset(data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb631ac1a9c994e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_data, batch_labels in dataloader:\n",
    "    print(\"Batch data shape:\", batch_data.shape)\n",
    "    print(\"Batch labels shape:\", batch_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7fd36218e2b52",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Note that the last batch may be smaller than the specified batch size if the total number of samples is not divisible by the batch size.\n",
    "* In practice, you would typically use built-in datasets from libraries like `torchvision` for image data or `torchtext` for text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c348b7adf793a72",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modules\n",
    "* Modules are the building blocks of neural networks in PyTorch.\n",
    "* You can create custom modules by subclassing `torch.nn.Module` and defining the `__init__` and `forward` methods.\n",
    "* Here is an example of a simple feedforward neural network module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165620a12e1d95d7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) # Fully connected layer \n",
    "        self.relu = nn.ReLU() # Activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # Fully connected layer \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b08a794155e7f94",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = SimpleNN(input_size=10, hidden_size=20, output_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a67229676b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eb5056c6e38c88",
   "metadata": {},
   "source": [
    "* You can combine multiple modules to create more complex architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4ed164",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Autograd\n",
    "* Autograd is PyTorch's automatic differentiation engine that computes gradients for tensor operations.\n",
    "* When you perform operations on tensors with `requires_grad=True`, PyTorch builds a computation graph to track these operations.\n",
    "* You can compute gradients by calling the `backward()` method on a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a094cdcd5f3ee7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Here is an example of using autograd to compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7188ec8986b15bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, requires_grad=True)\n",
    "print(\"Input tensor:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f9063a0ced54c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x ** 2 + 3 * x + 2\n",
    "print(\"Output tensor:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b974660da266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sum = y.sum()\n",
    "print(\"Sum of output tensor:\", y_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982af4c59bdd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sum.backward()\n",
    "print(\"Gradients:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbfbc35",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Optimisers\n",
    "* Optimisers are algorithms used to update model parameters based on computed gradients.\n",
    "* PyTorch provides several built-in optimisers in the `torch.optim` module, such as SGD and Adam.\n",
    "* You need to create an optimiser instance by passing the model parameters and learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ce060a32f475f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Here is an example of using the Adam optimiser to update model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1857049e449b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "model = SimpleNN(input_size=10, hidden_size=20, output_size=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "print(\"Initial model parameters:\")\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a444846546cba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create dummy input and target\n",
    "input_data = torch.randn(32, 10)\n",
    "target = torch.randint(0, 2, (32,))\n",
    "print(\"Input data shape:\", input_data.shape)\n",
    "print(\"Target shape:\", target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c829afea51cbbed0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "output = model(input_data)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31c8803ac6c825c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Compute loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, target)\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f63f4913b1bb50",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Backward pass and optimisation step\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(\"Updated model parameters:\")\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36308d18",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training and Validation Loop\n",
    "* A typical training loop involves iterating over the dataset, performing forward and backward passes, and updating model parameters.\n",
    "* After training, you should evaluate the model on a validation or test dataset to assess its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef72330cd2fc42b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Here is a simplified example of a training and validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61512d5234280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy input and target\n",
    "input_data = torch.randn(100, 10)\n",
    "target = torch.sum(input_data, dim=1).long() % 2  # Binary classification target\n",
    "print(\"Input data shape:\", input_data.shape)\n",
    "print(\"Target shape:\", target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75643f0e31560b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = SimpleDataset(input_data, target)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2481bebe0773e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model, criterion, and optimiser\n",
    "model = SimpleNN(input_size=10, hidden_size=20, output_size=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d1cdcc5c66084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    for batch_data, batch_labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_data)\n",
    "        loss = criterion(output, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for batch_data, batch_labels in dataloader:\n",
    "            output = model(batch_data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total_correct += (predicted == batch_labels).sum().item()\n",
    "            total_samples += batch_labels.size(0)\n",
    "        accuracy = total_correct / total_samples\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy * 100:.2f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6796e193febb2fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Saving and Loading Models\n",
    "* After training a model, you may want to save its parameters for later use.\n",
    "* PyTorch provides functions to save and load model state dictionaries.\n",
    "* Here is an example of saving and loading a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb5b557114b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"simple_nn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd3debe55c3b365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "loaded_model = SimpleNN(input_size=10, hidden_size=20, output_size=2)\n",
    "loaded_model.load_state_dict(torch.load(\"simple_nn.pth\"))\n",
    "print(\"Loaded model parameters:\")\n",
    "for param in loaded_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aa3fff25eeea38",
   "metadata": {},
   "source": [
    "## Example: Training a MLP classifier on MNIST dataset.\n",
    "* Let's put everything together and train a simple MLP classifier on the MNIST dataset using PyTorch.\n",
    "* We will use the `torchvision` library to load the MNIST dataset.\n",
    "* Note: In case you get errors when downloading the datasets, you can download them from [here](https://github.com/golbin/TensorFlow-MNIST/tree/master/mnist/data) and place them under `./mnist_data/MNIST/raw`.\n",
    "* Note: The training process may take some time depending on your hardware. You can prepare some coffee and enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1569b9b95332508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./mnist_data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./mnist_data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate model, criterion, and optimiser\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and validation loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_data)\n",
    "        loss = criterion(output, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            output = model(batch_data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total_correct += (predicted == batch_labels).sum().item()\n",
    "            total_samples += batch_labels.size(0)\n",
    "        accuracy = total_correct / total_samples\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy * 100:.2f}%.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19302fd1c5a16266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example predictions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "outputs = model(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "fig, axes = plt.subplots(2, 4, figsize=(10, 5))\n",
    "for i in range(8):\n",
    "    ax = axes[i // 4, i % 4]\n",
    "    ax.imshow(images[i].squeeze(), cmap='gray')\n",
    "    ax.set_title(f\"Pred: {predicted[i].item()}, True: {labels[i].item()}\")\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ea4510a319d38",
   "metadata": {},
   "source": [
    "## Common PyTorch Pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67700bf27dc3e3d1",
   "metadata": {},
   "source": [
    "* Mixing up CPU and GPU tensors, leading to runtime errors.\n",
    "* Here is an example that raises an error due to device mismatch.\n",
    "* Note: This example only behaves as expected if you have a CUDA-capable GPU and the CUDA version of PyTorch installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ee4ae7d68d2eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, device=\"cpu\")\n",
    "# y = torch.randn(5, device=\"cuda\") # NOTE, I don't have cuda installed on my Mac\n",
    "y = torch.randn(5, device=\"mps\") # This is the apple silicon device\n",
    "z = x + y  # RuntimeError: mismatched devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1a9b187ef086f",
   "metadata": {},
   "source": [
    "* Wrong data type for labels, especially in classification tasks (e.g., using float instead of long for class indices).\n",
    "* Here is an example that raises an error due to incorrect label data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d2b4a08351f963",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.randn(3, 5)  # Batch size 3, 5 classes\n",
    "labels = torch.tensor([1.0, 0.0, 4.0])  # Float tensor instead of Long\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, labels)  # RuntimeError: expected Long but got Float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b55c446589ad05",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Other common pitfalls include:\n",
    "  - Not setting up the correct mode with `model.train()` or `model.eval()`. This may affect layers like dropout and batch normalisation.\n",
    "  - Forgetting to call `optimizer.zero_grad()` before computing the loss, which will errorfully accumulate gradients.\n",
    "  - Forgetting to shuffle the training data. This may lead to poor generalisation.\n",
    "  - Using an inappropriate learning rate. This can cause slow convergence or divergence.\n",
    "  - Incorrectly reshaping tensors. This can cause dimension mismatches."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
